![X265](../videocodeciamges/X265.jpg)

[原图](../videocodeciamges/x265.jpg)

# main ()函数——解析函数参数并进行编码准备工作：x265.cpp

（1）GetConsoleTitle(orgConsoleTitle, CONSOLE_TITLE_SIZE)：获取控制台窗口
（2）cliopt.parse()函数：解析参数，直接调用可打印输入视频的详细信息
（3）api->encoder_open()函数：打开编码器配置
（4）api->encoder_headers()函数：设置NAL相关信息
（5）api->encoder_encode()函数：进入编码函数
（6）api->encoder_close()函数：结束编码并进行总结



## 1.clioptions 结构体分析



~~~c++
/* 命令行接口 */
struct CLIOptions
{
    InputFile*  input;          // 输入文件，抽象类, 定义在 \input\input.h 中
    ReconFile*  recon;          // 重构文件，抽象类，定义在 \output\output.h 中
    OutputFile* output;         // 输出文件，抽象类，定义在 \output\output.h 中
    FILE*       qpfile;         // 量化因子文件指针
    FILE*       csvfpt;         // csv日志文件指针
    const char* csvfn;          // csv日志文件名
    const char* reconPlayCmd;   
    const x265_api* api;        // 
    x265_param* param;          // x265编码器参数集
    bool bProgress;             // 是否输出编码进度和其他一些编码状态数据
    bool bForceY4m;             // 如果输入文件是Y4M格式，需要强制指定输入格式
    bool bDither;
    int csvLogLevel;            // csv日志级别，log level定义在 x265.h中
    uint32_t seek;              // 输入视频起始需要跳过的帧数
    uint32_t framesToBeEncoded; // 待编码的帧数
    uint64_t totalbytes;        // 已编码的数据字节数
    int64_t startTime;          // 起始编码时间点
    int64_t prevUpdateTime;     // 上一次编码信息输出的时间点

```
/* in microseconds，相邻两次编码状态输出的最小时间间隔，单位微妙 */
static const int UPDATE_INTERVAL = 250000;
 
// 构造函数，初始化上述各成员变量
CLIOptions()
{
    … … … … 
}
 
void destroy();
void printStatus(uint32_t frameNum);        // 打印编码状态信息
bool parse(int argc, char **argv);          // 解释命令行参数
bool parseQPFile(x265_picture &pic_org);    // 解释量化因子QP文件
```

};

/* 打印编码状态信息 */
void CLIOptions::printStatus(uint32_t frameNum)
{
    char buf[200];
    int64_t time = x265_mdate();

```
    // 是否输出编码状态信息取决于：进度开关、当前编码帧数、上次信息输出和当前时间的间隔
    if (!bProgress || !frameNum || (prevUpdateTime && time - prevUpdateTime < UPDATE_INTERVAL))
        return;

    // 计算编码帧率和码率
    int64_t elapsed = time - startTime;
    double fps = elapsed > 0 ? frameNum * 1000000. / elapsed : 0;
    float bitrate = 0.008f * totalbytes * (param->fpsNum / param->fpsDenom) / ((float)frameNum);
    if (framesToBeEncoded)
    {
        int eta = (int)(elapsed * (framesToBeEncoded - frameNum) / ((int64_t)frameNum * 1000000));
        sprintf(buf, "x265 [%.1f%%] %d/%d frames, %.2f fps, %.2f kb/s, 
            eta %d:%02d:%02d", 100. * frameNum / framesToBeEncoded, frameNum, 
            framesToBeEncoded, fps, bitrate, eta / 3600, (eta / 60) % 60, eta % 60);
    }
    else
        sprintf(buf, "x265 %d frames: %.2f fps, %.2f kb/s", frameNum, fps, bitrate);

    fprintf(stderr, "%s  \r", buf + 5);
    SetConsoleTitle(buf);
    fflush(stderr);             // needed in windows
    prevUpdateTime = time;


}
~~~

## 2.cliopt.parse(argc, argv) 

cliopt.parse(argc, argv)的主要功能是分析參数，直接调用x265.cpp中的bool CLIOptions::parse(int argc, char **argv)函数，该函数会打印输入视频的分辨率、帧率、视频格式、所要编码的帧数目以及输出文件名等。



## 3.parseQPFile(x265_picture &pic_org); 

// 解析量化因子QP文件

## 4.main流程

~~~c++
/*=============================================================*/
/* CLI return codes:
 *

- 0 - encode successful

- 1 - unable to parse command line

- 2 - unable to open encoder

- 3 - unable to generate stream headers

- 4 - encoder abort

- 5 - unable to open csv file
  *
   */
  int main(int argc, char **argv)	//主函数入口
  {
  #if HAVE_VLD
  // This uses Microsoft's proprietary WCHAR type, but this only builds on Windows to start with
  VLDSetReportOptions(VLD_OPT_REPORT_TO_DEBUGGER | VLD_OPT_REPORT_TO_FILE, L"x265_leaks.txt");
  #endif
  PROFILE_INIT();
  THREAD_NAME("API", 0);

  GetConsoleTitle(orgConsoleTitle, CONSOLE_TITLE_SIZE);	//获取控制台窗体
  SetThreadExecutionState(ES_CONTINUOUS | ES_SYSTEM_REQUIRED | ES_AWAYMODE_REQUIRED);

  ReconPlay* reconPlay = NULL;
  CLIOptions cliopt;

  if (cliopt.parse(argc, argv))	//==========分析參数。对编码器的參数进行设定，打开文件
  {
      cliopt.destroy();
      if (cliopt.api)
          cliopt.api->param_free(cliopt.param);
      exit(1);
  }

  x265_param* param = cliopt.param;
  const x265_api* api = cliopt.api;

  /* This allows muxers to modify bitstream format */
  cliopt.output->setParam(param);

  if (cliopt.reconPlayCmd)
      reconPlay = new ReconPlay(cliopt.reconPlayCmd, *param);

  /* note: we could try to acquire a different libx265 API here based on

  - the profile found during option parsing, but it must be done before
  - opening an encoder */

  x265_encoder *encoder = api->encoder_open(param);	//==========encoder_open()函数，打印编码器配置
  if (!encoder)	//若打不开编码器配置。提示错误
  {
      x265_log(param, X265_LOG_ERROR, "failed to open encoder\n");
      cliopt.destroy();
      api->param_free(param);
      api->cleanup();
      exit(2);
  }

  /* get the encoder parameters post-initialization */
  api->encoder_parameters(encoder, param);

  if (cliopt.csvfn)
  {
      cliopt.csvfpt = x265_csvlog_open(*api, *param, cliopt.csvfn, cliopt.csvLogLevel);
      if (!cliopt.csvfpt)
      {
          x265_log(param, X265_LOG_ERROR, "Unable to open CSV log file <%s>, aborting\n", cliopt.csvfn);
          cliopt.destroy();
          if (cliopt.api)
              cliopt.api->param_free(cliopt.param);
          exit(5);
      }
  }

  /* Control-C handler */
  //当键入Ctrl-C的时候，当前运行程序调用指针函数sigint_handler 运行完后，再返回原来运行的地方接着往下走。

if (signal(SIGINT, sigint_handler) == SIG_ERR)
        x265_log(param, X265_LOG_ERROR, "Unable to register CTRL+C handler: %s\n", strerror(errno));

```
x265_picture pic_orig, pic_out;		//定义x265的输入pic_orig和输出pic_out
x265_picture *pic_in = &pic_orig;	//获取x265的输入pic_orig的地址
 
/* Allocate recon picture if analysisMode is enabled */
std::priority_queue<int64_t>* pts_queue = cliopt.output->needPTS() ? new std::priority_queue<int64_t>() : NULL;
x265_picture *pic_recon = (cliopt.recon || !!param->analysisMode || pts_queue || reconPlay || cliopt.csvLogLevel) ?
```

&pic_out : NULL;

```
uint32_t inFrameCount = 0;	//输入的帧数
uint32_t outFrameCount = 0;	//输出的帧数
 
x265_nal *p_nal;
x265_stats stats;
uint32_t nal;
int16_t *errorBuf = NULL;
int ret = 0;
 
if (!param->bRepeatHeaders)
{
    if (api->encoder_headers(encoder, &p_nal, &nal) < 0)	//==========encoder_headers函数
    {
        x265_log(param, X265_LOG_ERROR, "Failure generating stream headers\n");
        ret = 3;
        goto fail;
    }
    else
        cliopt.totalbytes += cliopt.output->writeHeaders(p_nal, nal);
}
 
api->picture_init(param, pic_in);
 
if (cliopt.bDither)
{
    errorBuf = X265_MALLOC(int16_t, param->sourceWidth + 1);
    if (errorBuf)
        memset(errorBuf, 0, (param->sourceWidth + 1) * sizeof(int16_t));
    else
        cliopt.bDither = false;
}
 
// main encoder loop（编码主循环）
while (pic_in && !b_ctrl_c)
{
    pic_orig.poc = inFrameCount;
    if (cliopt.qpfile)
    {
        if (!cliopt.parseQPFile(pic_orig))
        {
            x265_log(NULL, X265_LOG_ERROR, "can't parse qpfile for frame %d\n", pic_in->poc);
            fclose(cliopt.qpfile);
            cliopt.qpfile = NULL;
        }
    }
 
	//当输入帧将要所有编码且输入的帧数大于或等于将要编码的帧数
    if (cliopt.framesToBeEncoded && inFrameCount >= cliopt.framesToBeEncoded)
        pic_in = NULL;
    else if (cliopt.input->readPicture(pic_orig))	//每读入一帧
        inFrameCount++;	//输入的帧数自加1
    else
        pic_in = NULL;
 
    if (pic_in)
    {
        if (pic_in->bitDepth > param->internalBitDepth && cliopt.bDither)
        {
            x265_dither_image(*api, *pic_in, cliopt.input->getWidth(), cliopt.input->getHeight(), errorBuf, param->internalBitDepth);
            pic_in->bitDepth = param->internalBitDepth;
        }
        /* Overwrite PTS */
        pic_in->pts = pic_in->poc;
    }
	//进行编码的入口函数，读入24帧后才開始编码
    int numEncoded = api->encoder_encode(encoder, &p_nal, &nal, pic_in, pic_recon);	//==========encoder_encode()函数。numEncoded是将要编码的帧数
    if (numEncoded < 0)
    {
        b_ctrl_c = 1;
        ret = 4;
        break;
    }
 
    if (reconPlay && numEncoded)
        reconPlay->writePicture(*pic_recon);
 
    outFrameCount += numEncoded;
 
    if (numEncoded && pic_recon && cliopt.recon)
        cliopt.recon->writePicture(pic_out);
    if (nal)
    {
        cliopt.totalbytes += cliopt.output->writeFrame(p_nal, nal, pic_out);
        if (pts_queue)
        {
            pts_queue->push(-pic_out.pts);
            if (pts_queue->size() > 2)
                pts_queue->pop();
        }
    }
 
    cliopt.printStatus(outFrameCount); //打印编码帧的详细信息
    if (numEncoded && cliopt.csvLogLevel)
        x265_csvlog_frame(cliopt.csvfpt, *param, *pic_recon, cliopt.csvLogLevel);
}
 
/* Flush the encoder */
/*功能：前面读入24帧后才開始编码，此处事实上就是处理相应的倒数的24帧，将其存储*/
while (!b_ctrl_c)	//退出上一个大循环后且没有按下Ctrl+C，代码继续运行
{
    //==========encoder_encode()函数
	int numEncoded = api->encoder_encode(encoder, &p_nal, &nal, NULL, pic_recon);
    if (numEncoded < 0)
    {
        ret = 4;
        break;
    }
 
    if (reconPlay && numEncoded)
        reconPlay->writePicture(*pic_recon);
 
    outFrameCount += numEncoded;
    if (numEncoded && pic_recon && cliopt.recon)
        cliopt.recon->writePicture(pic_out);
    if (nal)
    {
        cliopt.totalbytes += cliopt.output->writeFrame(p_nal, nal, pic_out);
        if (pts_queue)
        {
            pts_queue->push(-pic_out.pts);
            if (pts_queue->size() > 2)
                pts_queue->pop();
        }
    }
 
    cliopt.printStatus(outFrameCount);
    if (numEncoded && cliopt.csvLogLevel)
        x265_csvlog_frame(cliopt.csvfpt, *param, *pic_recon, cliopt.csvLogLevel);
 
    if (!numEncoded)
        break;
}
 
/* clear progress report */
if (cliopt.bProgress)
    fprintf(stderr, "%*s\r", 80, " ");
```

fail:

```
delete reconPlay;
 
api->encoder_get_stats(encoder, &stats, sizeof(stats));
if (cliopt.csvfpt && !b_ctrl_c)
    x265_csvlog_encode(cliopt.csvfpt, *api, *param, stats, cliopt.csvLogLevel, argc, argv);
api->encoder_close(encoder);	//==========encoder_close()函数
 
int64_t second_largest_pts = 0;
int64_t largest_pts = 0;
if (pts_queue && pts_queue->size() >= 2)
{
    second_largest_pts = -pts_queue->top();
    pts_queue->pop();
    largest_pts = -pts_queue->top();
    pts_queue->pop();
    delete pts_queue;
    pts_queue = NULL;
}
cliopt.output->closeFile(largest_pts, second_largest_pts);
 
if (b_ctrl_c)	//按下Ctrl+C，直接退出
    general_log(param, NULL, X265_LOG_INFO, "aborted at input frame %d, output frame %d\n",
                cliopt.seek + inFrameCount, stats.encodedPictureCount);
 
api->cleanup(); /* Free library singletons */
 
cliopt.destroy();
 
api->param_free(param);
 
X265_FREE(errorBuf);
 
SetConsoleTitle(orgConsoleTitle);	//设置控制窗体标题
SetThreadExecutionState(ES_CONTINUOUS);
```

#if HAVE_VLD
    assert(VLDReportLeaks() == 0);
#endif

```
return ret;
```

}
~~~





## encoder.cpp注释

在x265中，main()函数中调用了encoder_encode()函数，而encoder_encode()函数调用了encode()函数，encode()函数的主要功能是输入一帧图像，得到一帧图像的输出。

        encode()函数主要包括大致三个部分：

（1）分析是否由于错误造成的代码终止，如g_checkFailures、m_aborted。

（2）判断是否有输入帧，若有，则判断该输入帧的像素深度、颜色空间是否支持，并判断List是否为空，若为空则创建；除此之外，还有一个比较重要的变量，即ret，此处初始化了ret为0，ret用于判断encode()函数的执行状况，0代表当前没有可供输出的重构帧，则返回encoder_encode()函数进行处理，1代表有输出，从encode()函数的最后一行代码return ret可以证实这一点。

（3）用一个do/while()判断是否有输出，若有则ret为1，并且调用了startCompressFrame()函数，startCompressFrame()函数的主要目的就是触发线程，为进一步的编码做准备。


```c++
/**

- Feed one new input frame into the encoder, get one frame out. If pic_in is
- NULL, a flush condition is implied and pic_in must be NULL for all subsequent
- calls for this encoder instance.
  *
- pic_in  input original YUV picture or NULL
- pic_out pointer to reconstructed picture struct
  *
- returns 0 if no frames are currently available for output
- 1 if frame was output, m_nalList contains access unit
- negative on malloc error or abort */

//（1）分析是否由于错误造成的代码终止，如g_checkFailures、m_aborted。
int Encoder::encode(const x265_picture* pic_in, x265_picture* pic_out)
{
#if CHECKED_BUILD || _DEBUG
    if (g_checkFailures)
    {
        x265_log(m_param, X265_LOG_ERROR, "encoder aborting because of internal error\n");
        return -1;
    }
#endif
    if (m_aborted)
        return -1;
//输出图片吗？？？？
    if (m_exportedPic)
    {
        ATOMIC_DEC(&m_exportedPic->m_countRefEncoders);
        m_exportedPic = NULL;
        m_dpb->recycleUnreferenced();
    }
 2   //若有图片输入时
    if (pic_in)
    {
    //判断输入帧的颜色空间是否支持（（取样格式 4:4:4 4:2:0）），若不支持，打印错误 Unsupported color space on input
        if (pic_in->colorSpace != m_param->internalCsp)
        {
            x265_log(m_param, X265_LOG_ERROR, "Unsupported color space (%d) on input\n",
                     pic_in->colorSpace);
            return -1;//异常退出
        }
        //输入的每一帧的深度必须处于8至16范围内（检错像素深度） 否则 打印错误
        if (pic_in->bitDepth < 8 || pic_in->bitDepth > 16)
        {
            x265_log(m_param, X265_LOG_ERROR, "Input bit depth (%d) must be between 8 and 16\n",
                     pic_in->bitDepth);
            return -1;
        }
```



```c++
    Frame *inFrame;//create空间 用于存储视频帧
    //若List为空，则创建
    if (m_dpb->m_freeList.empty())
    {
        inFrame = new Frame; //创建空间
        //语法 A？X：Y     "A"为判断条件，“A"为真时为X，”A”不成立时为”Y"。
        x265_param* p = m_reconfigured? m_latestParam : m_param;//判断是否是重新配置的 ，否则选择新的配置文件
        if (inFrame->create(p)) //申请frame空间
        { //第一个创建的PicYuv被要求生成CU和块单元偏移量。数组用于与所有后续的PicYuv（原始和重建）共享。
            /* the first PicYuv created is asked to generate the CU and block unit offset
             * arrays which are then shared with all subsequent PicYuv (orig and recon) 
             * allocated by this top level encoder */
 
            if (m_cuOffsetY)
            {//将encoder offset指针赋值到对应m_fencPic对象中
            //m_cuOffsetY;  申请空间为一帧LCU个数，按照行列对应亮度LCU的pixel地址（ 每个CTU中三个颜色分量的偏移地址 ）
            //m_cuOffsetC; 申请空间为一帧LCU个数，按照行列对应色度LCU的pixel地址 ??有区别吗
            //m_buOffsetY;  申请空间为一个LCU的part个数（默认256个4x4），为当前亮度位置与LCU首地址的偏移地址 （CTU中每个4x4分量的偏移地址）
            //m_buOffsetC;  申请空间为一个LCU的part个数（默认256个4x4），为当前色度位置与LCU首地址的偏移地址 （CTU中每个4x4分量的偏移地址）
 
                inFrame->m_fencPic->m_cuOffsetC = m_cuOffsetC;
                inFrame->m_fencPic->m_cuOffsetY = m_cuOffsetY;
                inFrame->m_fencPic->m_buOffsetC = m_buOffsetC;
                inFrame->m_fencPic->m_buOffsetY = m_buOffsetY;
            }
            else
            {//申请偏移计算空间  如果是0偏移空间？？
                if (!inFrame->m_fencPic->createOffsets(m_sps))
                {// m_aborted报错设置true
                    m_aborted = true;
                    x265_log(m_param, X265_LOG_ERROR, "memory allocation failure, aborting encode\n");
                    inFrame->destroy();
                    delete inFrame;
                    return -1;//退出
                }
                else //申请偏移计算空间正常  正常赋值
                {
                    m_cuOffsetC = inFrame->m_fencPic->m_cuOffsetC;
                    m_cuOffsetY = inFrame->m_fencPic->m_cuOffsetY;
                    m_buOffsetC = inFrame->m_fencPic->m_buOffsetC;
                    m_buOffsetY = inFrame->m_fencPic->m_buOffsetY;
                }
            }
        }
        else //
        {
            m_aborted = true;
            x265_log(m_param, X265_LOG_ERROR, "memory allocation failure, aborting encode\n");
            inFrame->destroy();
            delete inFrame;
            return -1;
        }
    }
    else //列表非空的情况 则popBack  前面执行if (m_dpb->m_freeList.empty())
    {
        inFrame = m_dpb->m_freeList.popBack();
        inFrame->m_lowresInit = false; //标示初始化失败
    }
 
    /* Copy input picture into a Frame and PicYuv, send to lookahead */
    inFrame->m_fencPic->copyFromPicture(*pic_in, m_sps.conformanceWindow.rightOffset, m_sps.conformanceWindow.bottomOffset);
 
    inFrame->m_poc       = ++m_pocLast;//累加读入帧数 m_pocLast：当前已经读入的视频帧数（从0开始计数，初始化为-1）time index (POC)
    inFrame->m_userData  = pic_in->userData;
    inFrame->m_pts       = pic_in->pts;//累加读入帧数
    inFrame->m_forceqp   = pic_in->forceqp; //m_qp = (int32_t)(curFrame->m_forceqp + 0.5) - 1; ？是步进值吗
    inFrame->m_param     = m_reconfigured ? m_latestParam : m_param;//参数重置
 
   // 第一帧，其值等于最先进入的pts号(一般等于0) pts:进程
    if (m_pocLast == 0)
        m_firstPts = inFrame->m_pts;
   //m_bframeDelay 延迟帧数：p->bframes ? (p->bBPyramid ? 2 : 1) : 0;  一般等于2
    if (m_bframeDelay && m_pocLast == m_bframeDelay)
        m_bframeDelayTime = inFrame->m_pts - m_firstPts;// 计算延迟的pts号个数
 
    /* Encoder holds a reference count until stats collection is finished */
    //编码器持有一个引用计数，直到统计资料收集完成。
    //atomic_inc(&v)对变量增加1 （所谓原子操作，就是该操作绝不会在执行完毕前被任何其他任务或事件打断）
    ATOMIC_INC(&inFrame->m_countRefEncoders);将当前的被参考次数设置为1 防止后面被释放 此处值为1
    //??
    if ((m_param->rc.aqMode || m_param->bEnableWeightedPred || m_param->bEnableWeightedBiPred) &&
        (m_param->rc.cuTree && m_param->rc.bStatRead))
    {
        if (!m_rateControl->cuTreeReadFor2Pass(inFrame))
        {
            m_aborted = 1;
            return -1;
        }
    }
 
    /* Use the frame types from the first pass, if available */
    // 1pass中，如果没有经过parseQPFile(解析QPF文件)，则slicetyoe 为X265_TYPE_AUTO
    int sliceType = (m_param->rc.bStatRead) ? m_rateControl->rateControlSliceType(inFrame->m_poc) : pic_in->sliceType;
 
    /* In analysisSave mode, x265_analysis_data is allocated in pic_in and inFrame points to this */
    /* Load analysis data before lookahead->addPicture, since sliceType has been decided */
    //在analysisSave模式中，x265_analysis_data被分配到pic_in中，inFrame指向此
    //在lookahead->addPicture之前加载分析数据，因为sliceType已经确定
    if (m_param->analysisMode == X265_ANALYSIS_LOAD)
    {
        x265_picture* inputPic = const_cast<x265_picture*>(pic_in);
        /* readAnalysisFile reads analysis data for the frame and allocates memory based on slicetype */
        //readAnalysisFile 读取框架的分析数据，并根据slicetype分配内存。
        readAnalysisFile(&inputPic->analysisData, inFrame->m_poc);
        inFrame->m_analysisData.poc = inFrame->m_poc;
        inFrame->m_analysisData.sliceType = inputPic->analysisData.sliceType;
        inFrame->m_analysisData.numCUsInFrame = inputPic->analysisData.numCUsInFrame;
        inFrame->m_analysisData.numPartitions = inputPic->analysisData.numPartitions;
        inFrame->m_analysisData.interData = inputPic->analysisData.interData;
        inFrame->m_analysisData.intraData = inputPic->analysisData.intraData;
        sliceType = inputPic->analysisData.sliceType;
    }
 
    m_lookahead->addPicture(*inFrame, sliceType);//向输入列表中添加原始帧准备帧类型决策，在buffer满时，触发帧类型决策
    m_numDelayedPic++;//当前列表中有多少帧未编码 每当读入一帧++，每当编码完毕一帧减--
}
else//没有图输入
    m_lookahead->flush();//当前已经读取原始帧完毕，往后不用再继续读取，告知lookahead已满
 
FrameEncoder *curEncoder = m_frameEncoder[m_curEncoder];// 获取当前frameEncoder
m_curEncoder = (m_curEncoder + 1) % m_param->frameNumThreads;// 记录下一个frameEncoder
int ret = 0;//ret，此处初始化了ret为0，ret用于判断encode()函数的执行状况，0代表当前没有可供输出的重构帧，则返回encoder_encode()函数进行处理，1代表有输出，从encode()函数的最后一行代码return ret可以证实这一点。
```

 

```c++
/* Normal operation is to wait for the current frame encoder to complete its current frame
 * and then to give it a new frame to work on.  In zero-latency mode, we must encode this
 * input picture before returning so the order must be reversed. This do/while() loop allows
 * us to alternate the order of the calls without ugly code replication */
 //不等待返回当前帧 直接对下一个编码
Frame* outFrame = NULL;
Frame* frameEnc = NULL;
int pass = 0;//零延迟情况：有两个取值（0,1） 0表示读帧类型决定完毕的帧准备编码 1表示编码完毕写数据
do//循环功能：零延迟情况：pass=0 编码 pass =1 编码完毕写数据 循环两次   其它情况：只做一次 多线程控制编码与写数据
{
    /* getEncodedPicture() should block until the FrameEncoder has completed
     * encoding the frame.  This is how back-pressure through the API is
     * accomplished when the encoder is full */
    if (!m_bZeroLatency || pass)//pass=1 编码完毕
        outFrame = curEncoder->getEncodedPicture(m_nalList);//获取已经编码完毕的帧
    if (outFrame)//如果已经编码过
    {
        Slice *slice = outFrame->m_encData->m_slice;
        x265_frame_stats* frameData = NULL;
 
        /* Free up pic_in->analysisData since it has already been used */
        //释放pic_in->analysisData，因为它已经被使用了
        if (m_param->analysisMode == X265_ANALYSIS_LOAD)
            freeAnalysis(&outFrame->m_analysisData);
 
        if (pic_out)//pic_out是输出重构图像 有输出
        {
            PicYuv *recpic = outFrame->m_reconPic;
            pic_out->poc = slice->m_poc;
            pic_out->bitDepth = X265_DEPTH;
            pic_out->userData = outFrame->m_userData;
            pic_out->colorSpace = m_param->internalCsp;
            frameData = &(pic_out->frameData);
 
            pic_out->pts = outFrame->m_pts;
            pic_out->dts = outFrame->m_dts;
            //判断该帧的类型--I/P/B
            switch (slice->m_sliceType)
            {
            case I_SLICE:
                pic_out->sliceType = outFrame->m_lowres.bKeyframe ? X265_TYPE_IDR : X265_TYPE_I;
                break;
            case P_SLICE:
                pic_out->sliceType = X265_TYPE_P;
                break;
            case B_SLICE:
                pic_out->sliceType = X265_TYPE_B;
                break;
            }
 
            pic_out->planes[0] = recpic->m_picOrg[0];
            pic_out->stride[0] = (int)(recpic->m_stride * sizeof(pixel));
            pic_out->planes[1] = recpic->m_picOrg[1];
            pic_out->stride[1] = (int)(recpic->m_strideC * sizeof(pixel));
            pic_out->planes[2] = recpic->m_picOrg[2];
            pic_out->stride[2] = (int)(recpic->m_strideC * sizeof(pixel));
 
            /* Dump analysis data from pic_out to file in save mode and free */
            //在保存模式下将分析数据从pic_out转储到文件中，并释放出来
            if (m_param->analysisMode == X265_ANALYSIS_SAVE)
            {
                pic_out->analysisData.poc = pic_out->poc;
                pic_out->analysisData.sliceType = pic_out->sliceType;
                pic_out->analysisData.numCUsInFrame = outFrame->m_analysisData.numCUsInFrame;
                pic_out->analysisData.numPartitions = outFrame->m_analysisData.numPartitions;
                pic_out->analysisData.interData = outFrame->m_analysisData.interData;
                pic_out->analysisData.intraData = outFrame->m_analysisData.intraData;
                writeAnalysisFile(&pic_out->analysisData);
                freeAnalysis(&pic_out->analysisData);
            }
        }
        if (slice->m_sliceType == P_SLICE)
        {
            if (slice->m_weightPredTable[0][0][0].bPresentFlag)
                m_numLumaWPFrames++;
            if (slice->m_weightPredTable[0][0][1].bPresentFlag ||
                slice->m_weightPredTable[0][0][2].bPresentFlag)
                m_numChromaWPFrames++;
        }
        else if (slice->m_sliceType == B_SLICE)
        {
            bool bLuma = false, bChroma = false;
            for (int l = 0; l < 2; l++)
            {
                if (slice->m_weightPredTable[l][0][0].bPresentFlag)
                    bLuma = true;
                if (slice->m_weightPredTable[l][0][1].bPresentFlag ||
                    slice->m_weightPredTable[l][0][2].bPresentFlag)
                    bChroma = true;
            }
 
            if (bLuma)
                m_numLumaWPBiFrames++;
            if (bChroma)
                m_numChromaWPBiFrames++;
        }
 
        if (m_aborted)
            return -1;
 
        finishFrameStats(outFrame, curEncoder, curEncoder->m_accessUnitBits, frameData);
 
        /* Write RateControl Frame level stats in multipass encodes */
        if (m_param->rc.bStatWrite)
            if (m_rateControl->writeRateControlFrameStats(outFrame, &curEncoder->m_rce))
                m_aborted = true;
 
        /* Allow this frame to be recycled if no frame encoders are using it for reference */
        //如果没有帧编码器使用该帧作为参考，允许该帧被回收。
        if (!pic_out) //没有输出图像
        {
            ATOMIC_DEC(&outFrame->m_countRefEncoders);
            m_dpb->recycleUnreferenced();
        }
        else
            m_exportedPic = outFrame;
 
        m_numDelayedPic--;
 
        ret = 1;//有输出，则ret为1
    }
 
    /* pop a single frame from decided list, then provide to frame encoder
     * curEncoder is guaranteed to be idle at this point */
    if (!pass) //pass=0 有可用帧的时候才会进入 开始编码
        frameEnc = m_lookahead->getDecidedPicture();//获取已经得到帧类型的原始帧
    if (frameEnc && !pass)//pass=0 有可用帧的时候才会进入 开始编码
    {
        /* give this frame a FrameData instance before encoding */
        if (m_dpb->m_picSymFreeList)
        {
            frameEnc->m_encData = m_dpb->m_picSymFreeList;
            m_dpb->m_picSymFreeList = m_dpb->m_picSymFreeList->m_freeListNext;
            frameEnc->reinit(m_sps);
        }
        else
        {
            frameEnc->allocEncodeData(m_param, m_sps);//申请重构帧内存并初始化为0，申请一帧CTU的存储空间，初始化CTU、初始化统计信息
            Slice* slice = frameEnc->m_encData->m_slice;//获取slice指针
            slice->m_sps = &m_sps;//获取sps指针
            slice->m_pps = &m_pps;
            slice->m_maxNumMergeCand = m_param->maxNumMergeCand;//获取配置的Merge选择的候选个数
            slice->m_endCUAddr = slice->realEndAddress(m_sps.numCUsInFrame * NUM_4x4_PARTITIONS);//一帧中最后实际像素在帧中的4x4块标号+1
            frameEnc->m_reconPic->m_cuOffsetC = m_cuOffsetC;//将encoder offset指针赋值到对应frameEnc->m_reconPic对象中
            frameEnc->m_reconPic->m_cuOffsetY = m_cuOffsetY;
            frameEnc->m_reconPic->m_buOffsetC = m_buOffsetC;
            frameEnc->m_reconPic->m_buOffsetY = m_buOffsetY;
        }
 
        curEncoder->m_rce.encodeOrder = m_encodedFrameNum++;//获取当前编码顺序（从0开始计数）
        //计算有无延迟的dts
        if (m_bframeDelay)//有延迟 （获取DTS display time stamp 显示时间戳）
        {
            int64_t *prevReorderedPts = m_prevReorderedPts;
            //dts在开始不多于延迟帧数的时候需要计算，其它直接从数组中获
            frameEnc->m_dts = m_encodedFrameNum > m_bframeDelay
                ? prevReorderedPts[(m_encodedFrameNum - m_bframeDelay) % m_bframeDelay]
                : frameEnc->m_reorderedPts - m_bframeDelayTime;
            prevReorderedPts[m_encodedFrameNum % m_bframeDelay] = frameEnc->m_reorderedPts;
        }
        else
            frameEnc->m_dts = frameEnc->m_reorderedPts;//零延迟：解码顺序等于编码顺序
 
        /* Allocate analysis data before encode in save mode. This is allocated in frameEnc */
        if (m_param->analysisMode == X265_ANALYSIS_SAVE)
        {
            x265_analysis_data* analysis = &frameEnc->m_analysisData;
            analysis->poc = frameEnc->m_poc;
            analysis->sliceType = frameEnc->m_lowres.sliceType;
            uint32_t widthInCU       = (m_param->sourceWidth  + g_maxCUSize - 1) >> g_maxLog2CUSize;
            uint32_t heightInCU      = (m_param->sourceHeight + g_maxCUSize - 1) >> g_maxLog2CUSize;
 
            uint32_t numCUsInFrame   = widthInCU * heightInCU;
            analysis->numCUsInFrame  = numCUsInFrame;
            analysis->numPartitions  = NUM_4x4_PARTITIONS;
            allocAnalysis(analysis);
        }
 
        /* determine references, setup RPS, etc */
        //设置NAL单元类型，将待编码帧加入dpb（解码图片缓存区，不用于参考的图像输出后会被移除出DPB）列表，获取slice参考帧列表等slice参量，将该帧的参考帧的被参考次数加一
        m_dpb->prepareEncode(frameEnc);//准备编码前的一些工作，如决定参考帧,设置RPS等等
        //三种码率控制模式 qp模式…bitrate模式..crf模式
        //使用qp选项时，表示P帧的量化值为qp。I帧和B帧的量化值则是从--ipratio和--pbratio中取得
        if (m_param->rc.rateControlMode != X265_RC_CQP)//如果当前不是固定QP模式
            m_lookahead->getEstimatedPictureCost(frameEnc);//获取当前帧每个CTU行对应下采样帧的每个8x8的块cost的累计值
 
        /* Allow FrameEncoder::compressFrame() to start in the frame encoder thread */
        if (!curEncoder->startCompressFrame(frameEnc)) //编码线程的开始，调用了startCompressFrame()函数，而startCompressFrame()调用了m_enable.trigger()以触发线程  
            m_aborted = true;//报错？
    }
    else if (m_encodedFrameNum)//？？？零延迟情况：只有在pass=0？？？  其它情况：一般不进入？？？
        m_rateControl->setFinalFrameCount(m_encodedFrameNum); 
}
while (m_bZeroLatency && ++pass < 2);//循环功能：零延迟情况：pass=0 编码 pass =1 编码完毕写数据 循环两次   其它情况：只做一次 多线程控制编码与写数据
 
return ret;
```
}



## sao流程



![X265_SAO](../videocodeciamges/X265_SAO.png)






    

```c++
/* 对|num / den|四舍五入，然后前面添加符号 */
inline int32_t roundIBDI(int32_t num, int32_t den)
{
    return num >= 0 ? ((num * 2 + den)/(den * 2)) : -((-num * 2 + den)/(den * 2));
}
 
/* 获取输入变量x的符号 */
inline int8_t signOf(int x)
{
    return (x >> 31) | ((int)((((uint32_t)-x)) >> 31));
}
 
/* a等于b返回0， a小于b就返回-1，a大于b就返回1 */
inline int signOf2(const int a, const int b)
{
    int r = 0;
    if (a < b)  r = -1;
    if (a > b)  r = 1;
    return r;
}
 
/**
 * @brief 计算D_post和 D_pre的差值，其中D_pre和D_post分别表示原始像素与重构像素（SAO补偿前、补偿后）之间的失真。
 * @param count  : 一个CTB内某个特定SAO类型样本的个数
 * @param offset : 一个CTB内某个特定SAO类型样本的补偿值
 * @param offsetOrg : 原始像素与重构像素（SAO补偿前）之间的差值之和
 */
inline int64_t estSaoDist(int32_t count, int32_t offset, int32_t offsetOrg)
{
    return (count * offset - offsetOrg * 2) * offset;
}
 
/**
 * @brief 边界补偿模式下像素的5种分类 ：
 *        第1类谷点和第2类凹拐点，需要加上一个正补偿值；
 *        第4类峰点和第3类凸拐点，需要加上一个负补偿值；
 *        第0类像素不进行补偿。
 */
const uint32_t SAO::s_eoTable[NUM_EDGETYPE] =
{
    1, // 0
    2, // 1
    0, // 2
    3, // 3
    4  // 4
};
 
/**
 * @brief 创建SAO的部分参数
 */
bool SAO::create(x265_param* param, int initCommon)
{
    m_param = param;                        // 编码器参数集
    m_chromaFormat = param->internalCsp;    // 内部图像颜色空间，此处只考虑 I420
 
    // 色度水平和垂直方向移动的位数，对于I420格式的图像，此处都是1
    m_hChromaShift = CHROMA_H_SHIFT(param->internalCsp);
    m_vChromaShift = CHROMA_V_SHIFT(param->internalCsp);
 
    // 计算水平和垂直方向CU（编码单元）的个数，长度不足 g_maxCUSize 也算一个；
    // maxCUSize 表示CU的最大尺寸，此处取值为 64.
    m_numCuInWidth =  (m_param->sourceWidth + g_maxCUSize - 1) / g_maxCUSize;
    m_numCuInHeight = (m_param->sourceHeight + g_maxCUSize - 1) / g_maxCUSize;
 
    // maxY表示亮度的最大值，对于8位深度的图像来说，该最大值为255；
    // rangeExt 扩展范围为最大值的一半，此处为127；
    // numCtu 表示一帧中 CU （编码单元）的个数.
    const pixel maxY = (1 << X265_DEPTH) - 1;
    const pixel rangeExt = maxY >> 1;
    int numCtu = m_numCuInWidth * m_numCuInHeight;
 
    // 为当前CU的左边和上面CU申请空间，备份左边和上面CU主要用于预测当前CU；
    for (int i = 0; i < (param->internalCsp != X265_CSP_I400 ? 3 : 1); i++)
    {
        CHECKED_MALLOC(m_tmpL1[i], pixel, g_maxCUSize + 1);
        CHECKED_MALLOC(m_tmpL2[i], pixel, g_maxCUSize + 1);
 
        // SAO asm code will read 1 pixel before and after, so pad by 2
        // NOTE: m_param->sourceWidth+2 enough, to avoid condition check in 
        // copySaoAboveRef(), I alloc more up to 63 bytes in here
        CHECKED_MALLOC(m_tmpU[i], pixel, m_numCuInWidth * g_maxCUSize + 2 + 32);
        m_tmpU[i] += 1;
    }
 
    if (initCommon)
    {
        // 选择SAO方法处理去方块边界像素，如果开启则处理所有边界像素，
        // 关闭则不处理右边和下面边界的像素；缺省是关闭。
        if (m_param->bSaoNonDeblocked)
        {
            CHECKED_MALLOC(m_countPreDblk, PerPlane, numCtu);
            CHECKED_MALLOC(m_offsetOrgPreDblk, PerPlane, numCtu);
        }
        CHECKED_MALLOC(m_depthSaoRate, double, 2 * SAO_DEPTHRATE_SIZE);
 
        m_depthSaoRate[0 * SAO_DEPTHRATE_SIZE + 0] = 0;
        m_depthSaoRate[0 * SAO_DEPTHRATE_SIZE + 1] = 0;
        m_depthSaoRate[0 * SAO_DEPTHRATE_SIZE + 2] = 0;
        m_depthSaoRate[0 * SAO_DEPTHRATE_SIZE + 3] = 0;
        m_depthSaoRate[1 * SAO_DEPTHRATE_SIZE + 0] = 0;
        m_depthSaoRate[1 * SAO_DEPTHRATE_SIZE + 1] = 0;
        m_depthSaoRate[1 * SAO_DEPTHRATE_SIZE + 2] = 0;
        m_depthSaoRate[1 * SAO_DEPTHRATE_SIZE + 3] = 0;
 
        CHECKED_MALLOC(m_clipTableBase,  pixel, maxY + 2 * rangeExt);
        m_clipTable = &(m_clipTableBase[rangeExt]);
 
        // 创建一个快速查找表m_clipTable（用于补偿，限制越界），即:
        // {0, 0, ..., 0 (127个); 0, 1, 2, ..., 255; 255, 255, ..., 255（127个）}
        for (int i = 0; i < rangeExt; i++)
            m_clipTableBase[i] = 0;
        for (int i = 0; i < maxY; i++)
            m_clipTable[i] = (pixel)i;
        for (int i = maxY; i < maxY + rangeExt; i++)
            m_clipTable[i] = maxY;
    }
    else
    {
        // must initialize these common pointer outside of function
        m_countPreDblk = NULL;
        m_offsetOrgPreDblk = NULL;
        m_clipTableBase = NULL;
        m_clipTable = NULL;
    }
    return true;
fail:
    return false;
}
 
/* 为当前CTU的SAO参数分配空间并初始化 */
void SAO::allocSaoParam(SAOParam* saoParam) const
{
    int planes = (m_param->internalCsp != X265_CSP_I400) ? 3 : 1;
    saoParam->numCuInWidth  = m_numCuInWidth;
 
    for (int i = 0; i < planes; i++)
        saoParam->ctuParam[i] = new SaoCtuParam[m_numCuInHeight * m_numCuInWidth];
}
 
/**
 * @brief  根据SAO补偿模式对重构像素值进行补偿.
 * @param addr : 从上到下、从左到右，当前CTU的序号
 * @param typeIdx : SAO补偿模式，取值SAO_EO_X 或 SAO_BO
 * @param plane : 颜色空间平面的序号，亮度平面为0，两个色度平面分别为1和2. 
 */
void SAO::applyPixelOffsets(int addr, int typeIdx, int plane)
{
    // reconPic为YUV重构图像，rec为当面颜色平面当前CTU的在重构图像中起始地址
    PicYuv* reconPic = m_frame->m_reconPic; 
    pixel* rec = reconPic->getPlaneAddr(plane, addr);
 
    // 获取重构图像颜色平面对应的跨度，亮度和色度的跨度不一样
    intptr_t stride = plane ? reconPic->m_strideC : reconPic->m_stride;
    uint32_t picWidth  = m_param->sourceWidth;      // 原始图像的宽
    uint32_t picHeight = m_param->sourceHeight;     // 原始图像的高
    const CUData* cu = m_frame->m_encData->getPicCTU(addr);
    int ctuWidth  = g_maxCUSize;        // 当前CU的宽度
    int ctuHeight = g_maxCUSize;        // 当前CU的高度
 
    // 当前CU最左边的横坐标和最上面的纵坐标
    uint32_t lpelx = cu->m_cuPelX;
    uint32_t tpely = cu->m_cuPelY;
 
    // 如果是色度平面，相应的宽度和高度都要减半，即左移一位
    if (plane)
    {
        picWidth  >>= m_hChromaShift;
        picHeight >>= m_vChromaShift;
        ctuWidth  >>= m_hChromaShift;
        ctuHeight >>= m_vChromaShift;
        lpelx     >>= m_hChromaShift;
        tpely     >>= m_vChromaShift;
    }
 
    // 获取当前CU最右边和最下面的边界值，不超出原始图像的最右边和最下面
    uint32_t rpelx = x265_min(lpelx + ctuWidth,  picWidth);
    uint32_t bpely = x265_min(tpely + ctuHeight, picHeight);
 
    // 当前CU实际的宽度和高度，除了最右边和最下面的CU外，其他都是64x64
    ctuWidth  = rpelx - lpelx;
    ctuHeight = bpely - tpely;
 
    int8_t _upBuff1[MAX_CU_SIZE + 2], *upBuff1 = _upBuff1 + 1, signLeft1[2];
    int8_t _upBufft[MAX_CU_SIZE + 2], *upBufft = _upBufft + 1;
 
    memset(_upBuff1 + MAX_CU_SIZE, 0, 2 * sizeof(int8_t)); 
 
    pixel* tmpL = m_tmpL1[plane];
    pixel* tmpU = &(m_tmpU[plane][lpelx]);
    int8_t* offsetEo = m_offsetEo[plane];
 
    // 根据边界或边带类型进行相应的SAO补偿
    switch (typeIdx)
    {
        case SAO_EO_0: // dir: -
            {... ...}
        case SAO_EO_1: // dir: |
            {... ...}
        case SAO_EO_2: // dir: 135
            {... ...}
        case SAO_EO_3: // dir: 45
            {... ...}
        case SAO_BO:   // 边带补偿
            {... ...}
        default: break;
    }
}
 
/* 生成亮度CTU的各种模式下的SAO补偿值并进行补偿 */
void SAO::generateLumaOffsets(SaoCtuParam* ctuParam, int idxY, int idxX)
{
    PicYuv* reconPic = m_frame->m_reconPic;
    intptr_t stride = reconPic->m_stride;
    int ctuWidth  = g_maxCUSize;
    int ctuHeight = g_maxCUSize;
 
    // 根据idxX和idxY得到CTU的序号，再根据序号获取CTU在重构图像缓冲区中起始位置
    int addr = idxY * m_numCuInWidth + idxX;
    pixel* rec = reconPic->getLumaAddr(addr);
 
    // 如果是水平方向第一个CTU，则用m_tmpL1[0]保存CTU左边一列（即左边CTU最右边的一列，
    // 不属于该CTU）的重构值
    if (idxX == 0)
    {
        for (int i = 0; i < ctuHeight + 1; i++)
        {
            m_tmpL1[0][i] = rec[0];
            rec += stride;
        }
    }
 
    // 判断当前CTU是否与左边CTU的SAO模式一样
    bool mergeLeftFlag = (ctuParam[addr].mergeMode == SAO_MERGE_LEFT);
    int typeIdx = ctuParam[addr].typeIdx;
 
    // 当前CTU不是水平方向的最后一个CTU，则用m_tmpL2[0]来保存当前CTU最右边的一列
    //（属于该CTU）重构值,后续跟m_tmpL1[0]交换可以用于下一个CTU的SAO模式计算
    if (idxX != (m_numCuInWidth - 1))
    {
        rec = reconPic->getLumaAddr(addr);
        for (int i = 0; i < ctuHeight + 1; i++)
        {
            m_tmpL2[0][i] = rec[ctuWidth - 1];
            rec += stride;
        }
    }
 
    // SAO补偿模式总共五种，取值0 – 4.
    if (typeIdx >= 0)
    {
        // 如果跟左边的CTU相同的SAO模式，则 m_offsetEo 直接采用左边CTU的值
        if (!mergeLeftFlag)
        {
            if (typeIdx == SAO_BO)
            {
                memset(m_offsetBo[0], 0, sizeof(m_offsetBo[0]));
 
                for (int i = 0; i < SAO_NUM_OFFSET; i++)
                    m_offsetBo[0][((ctuParam[addr].bandPos + i) & (MAX_NUM_SAO_CLASS - 1))] = 
                                            (int8_t)(ctuParam[addr].offset[i] << SAO_BIT_INC);
            }
            else // 边界补偿，即SAO_EO_X, X = 0,1,2,3
            {
                int offset[NUM_EDGETYPE];
                offset[0] = 0;
                for (int i = 0; i < SAO_NUM_OFFSET; i++)
                    offset[i + 1] = ctuParam[addr].offset[i] << SAO_BIT_INC;
 
                for (int edgeType = 0; edgeType < NUM_EDGETYPE; edgeType++)
                    m_offsetEo[0][edgeType] = (int8_t)offset[s_eoTable[edgeType]];
            }
        }
 
        // m_offsetEo[0]保存了亮度平面各种边界或边带需要补偿的值,将该值用到SAO补偿中
        applyPixelOffsets(addr, typeIdx, 0);
    }
 
    // 交换m_tmpL1[0]与m_tmpL2[0]，就得到下一个CTU左边一列的重构值，即：m_tmpL1[0]
    std::swap(m_tmpL1[0], m_tmpL2[0]);
}
 
/* 生成色度CTU的各种模式下的SAO补偿值并进行补偿*/
void SAO::generateChromaOffsets(SaoCtuParam* ctuParam[3], int idxY, int idxX);
 
/* 统计当前CTU在BO和EO各模式下的像素归类，包括重构像素与原始像素差值之和，以及对classIdx的计数 */
void SAO::calcSaoStatsCTU(int addr, int plane)
{
    const PicYuv* reconPic = m_frame->m_reconPic;
    const CUData* cu = m_frame->m_encData->getPicCTU(addr);
    const pixel* fenc0 = m_frame->m_fencPic->getPlaneAddr(plane, addr);
    const pixel* rec0  = reconPic->getPlaneAddr(plane, addr);
    const pixel* fenc;
    const pixel* rec;
 
    // 亮度和色度平面的跨度不一样，plane为0表示亮度，非0表示色度
    intptr_t stride = plane ? reconPic->m_strideC : reconPic->m_stride;
    uint32_t picWidth  = m_param->sourceWidth;
    uint32_t picHeight = m_param->sourceHeight;
    int ctuWidth  = g_maxCUSize;
    int ctuHeight = g_maxCUSize;
    uint32_t lpelx = cu->m_cuPelX;      // 当前CTU最左边像素的横坐标
    uint32_t tpely = cu->m_cuPelY;      // 当前CTU最上面像素的纵坐标
 
    // 色度平面，相应的值都要减半，即左移一位
    if (plane)
    {
        picWidth  >>= m_hChromaShift;
        picHeight >>= m_vChromaShift;
        ctuWidth  >>= m_hChromaShift;
        ctuHeight >>= m_vChromaShift;
        lpelx     >>= m_hChromaShift;
        tpely     >>= m_vChromaShift;
    }
 
    // 当前CTU最右边像素的横坐标、最下面像素的纵坐标
    uint32_t rpelx = x265_min(lpelx + ctuWidth,  picWidth);
    uint32_t bpely = x265_min(tpely + ctuHeight, picHeight);
 
    // 当前CTU实际的宽度和高度，除了最右边和最下面的CTU外，其他CTU一般都是64x64
    ctuWidth  = rpelx - lpelx;
    ctuHeight = bpely - tpely;
 
    int startX, startY, endX, endY;
    const int plane_offset = plane ? 2 : 0;
    int skipB = 4, skipR = 5;
 
    int8_t _upBuff[2 * (MAX_CU_SIZE + 16 + 16)], *upBuff1 = _upBuff + 16, 
            *upBufft = upBuff1 +(MAX_CU_SIZE + 16 + 16);
    ALIGN_VAR_32(int16_t, diff[MAX_CU_SIZE * MAX_CU_SIZE]);
 
    // 计算 (fenc - frec)，结果放入diff中，即原始像素与重构像素间的失真
    if ((lpelx + ctuWidth <  picWidth) & (tpely + ctuHeight < picHeight))
    {
        // WARNING: *) May read beyond bound on video than ctuWidth or 
        // ctuHeight is NOT multiple of cuSize
        X265_CHECK((ctuWidth == ctuHeight) || (m_chromaFormat != X265_CSP_I420), 
                    "video size check failure\n");
 
        // 对于square的CU可以采用SIMD流指令计算(fenc - frec),
        // 此处 MAX_CU_SIZE = 64 可以看作 diff的跨度，fenc0和rec0的跨度都是stride
        if (plane)
            primitives.chroma[m_chromaFormat].cu[g_maxLog2CUSize - 2].
                        sub_ps(diff, MAX_CU_SIZE, fenc0, rec0, stride, stride);
        else
           primitives.cu[g_maxLog2CUSize - 2].sub_ps(diff, MAX_CU_SIZE, fenc0, rec0, stride, stride);
    }
    else
    {
        // path for non-square area (most in edge)
        // 最右边或最下面的CTU可能不是square，另外单独计算 (fenc - frec)
        for(int y = 0; y < ctuHeight; y++)
        {
            for(int x = 0; x < ctuWidth; x++)
            {
                diff[y * MAX_CU_SIZE + x] = (fenc0[y * stride + x] – rec0[y * stride + x]);
            }
        }
    }
 
    // SAO_BO:
    {
        // 缺省是disable，表示右边和底部边界不做去方块滤波
        if (m_param->bSaoNonDeblocked)
        {
            skipB = 3;
            skipR = 4;
        }
 
        endX = (rpelx == picWidth) ? ctuWidth : ctuWidth - skipR + plane_offset;
        endY = (bpely == picHeight) ? ctuHeight : ctuHeight - skipB + plane_offset;
 
        // 当前CTU按照BO补偿模式对像素进行归类，
        // 包括每个条带像素个数、原始像素与重构像素差值之和
        primitives.saoCuStatsBO(diff, rec0, stride, endX, endY, 
                        m_offsetOrg[plane][SAO_BO], m_count[plane][SAO_BO]);
    }
 
    // SAO_EO_0: // dir: -
    {
        if (m_param->bSaoNonDeblocked)  // 缺省是disable, 忽略
        {
             skipB = 3;
             skipR = 5;
        }
 
        startX = !lpelx;
        endX   = (rpelx == picWidth) ? ctuWidth - 1 : ctuWidth - skipR + plane_offset;
 
        // 当前CTU按照 EO_0 模式对像素进行归类
        primitives.saoCuStatsE0(diff + startX, rec0 + startX, stride, endX - startX, ctuHeight - skipB + 
                                plane_offset, m_offsetOrg[plane][SAO_EO_0], m_count[plane][SAO_EO_0]);
    }
 
    // SAO_EO_1: // dir: |
    {
        if (m_param->bSaoNonDeblocked)      // 缺省是disable, 忽略
        {
             skipB = 4;
             skipR = 4;
        }
 
        rec  = rec0;
 
        // 如果tpely = 0，就表示当前CTU位于最上方，因此从CTU的第二行开始进行统计
        startY = !tpely;
        endX   = (rpelx == picWidth) ? ctuWidth : ctuWidth - skipR + plane_offset;
        endY   = (bpely == picHeight) ? ctuHeight - 1 : ctuHeight - skipB + plane_offset;
        if (!tpely)
            rec += stride;      // 当前CTU第二行起始地址，为下面的sign计算做准备
 
        // 计算当前CTU第二行与第一行的像素差值，保存在 upBuff1 中
        primitives.sign(upBuff1, rec, &rec[- stride], ctuWidth);
 
        // 当前CTU按照 EO_1 模式对像素进行归类
        primitives.saoCuStatsE1(diff + startY * MAX_CU_SIZE, rec0 + startY * stride, stride, upBuff1, 
                           endX, endY - startY, m_offsetOrg[plane][SAO_EO_1], m_count[plane][SAO_EO_1]);
    }
 
    // SAO_EO_2: // dir: 135
    {
        if (m_param->bSaoNonDeblocked)      // 缺省是disable, 忽略
        {
             skipB = 4;
             skipR = 5;
        }
 
        fenc = fenc0;
        rec  = rec0;
 
        // 要计算某个像素与左上方像素（即135度方向）的差值，要确保左上方像素存在，
        // 因此如果CTU位于图像的最左边或最上方，startX、startY需为1
        startX = !lpelx;
        endX   = (rpelx == picWidth) ? ctuWidth - 1 : ctuWidth - skipR + plane_offset;
        startY = !tpely;
        endY   = (bpely == picHeight) ? ctuHeight - 1 : ctuHeight - skipB + plane_offset;
        if (!tpely)
        {
           fenc += stride;  // 原始图像第二行起始地址 
           rec  += stride;  // 当前CTU第二行起始地址，为下面的sign计算做准备
        }
 
        // 计算当前CTU第二行与第一行的像素差值(即与左上方像素的差值)，保存在 upBuff1 中
        primitives.sign(upBuff1, &rec[startX], &rec[startX - stride - 1], (endX - startX));
 
        // 当前CTU按照 EO_2 模式对像素进行归类
        primitives.saoCuStatsE2(diff + startX + startY * MAX_CU_SIZE, rec0  + startX + startY * stride, 
                            stride, upBuff1, upBufft, endX - startX, endY - startY, 
                            m_offsetOrg[plane][SAO_EO_2], m_count[plane][SAO_EO_2]);
    }
 
    // SAO_EO_3: // dir: 45
    {
        if (m_param->bSaoNonDeblocked)  // 缺省是disable, 忽略
        {
           skipB = 4;
           skipR = 5;
        }
 
        fenc = fenc0;
        rec  = rec0;
 
        startX = !lpelx;
        endX   = (rpelx == picWidth) ? ctuWidth - 1 : ctuWidth - skipR + plane_offset;
        startY = !tpely;
        endY   = (bpely == picHeight) ? ctuHeight - 1 : ctuHeight - skipB + plane_offset;
 
        if (!tpely)
        {
           fenc += stride;      // 原始图像第二行起始地址
           rec  += stride;      // 当前CTU第二行起始地址，为下面的sign计算做准备
        }
 
        // 计算当前CTU第二行与第一行的像素差值(即与右上方像素的差值)，保存在 upBuff1 中
        primitives.sign(upBuff1, &rec[startX - 1], &rec[startX - 1 - stride + 1],(endX - startX + 1));
 
        // 当前CTU按照 EO_3 模式对像素进行归类
        primitives.saoCuStatsE3(diff + startX + startY * MAX_CU_SIZE, rec0  + startX + startY * stride, 
                            stride, upBuff1 + 1, endX - startX, endY - startY, 
                            m_offsetOrg[plane][SAO_EO_3], m_count[plane][SAO_EO_3]);
    }
}
 
/* 去方块滤波前对CTU的像素统计归类，只有当SAO和bSaoNonDeblocked都开启的情况下才使用，因此暂时忽略 */
void SAO::calcSaoStatsCu_BeforeDblk(Frame* frame, int idxX, int idxY);
 
/* 计算CTU在各种模式下的最优SAO代价，与直接采用左边或上面CTU的SAO参数作比较，找出最优的SAO代价，
   并将最优SAO模式下的各种参数保存在 saoParam->ctuParam[plane][add]中 */
void SAO::rdoSaoUnitCu(SAOParam* saoParam, int rowBaseAddr, int idxX, int addr)
{
    Slice* slice = m_frame->m_encData->m_slice;
    const CUData* cu = m_frame->m_encData->getPicCTU(addr);
    int qp = cu->m_qp[0];
    int64_t lambda[2] = { 0 };
    int qpCb = qp;
 
    // 色度量化因子qpCb
    if (m_param->internalCsp == X265_CSP_I420)
        qpCb = x265_clip3(QP_MIN, QP_MAX_MAX, (int)g_chromaScale[qp + slice->m_pps->chromaQpOffset[0]]);
    else
        qpCb = X265_MIN(qp + slice->m_pps->chromaQpOffset[0], QP_MAX_SPEC);
 
    // lambda[0]用于亮度SAO参数计算，lambda[1]用于色度SAO参数计算
    lambda[0] = (int64_t)floor(256.0 * x265_lambda2_tab[qp]);
    lambda[1] = (int64_t)floor(256.0 * x265_lambda2_tab[qpCb]); 
 
    // 左边和上面的CU是否存在
    const bool allowMerge[2] = {(idxX != 0), (rowBaseAddr != 0)}; 
 
    // 左边和上面的CU的编号
    const int addrMerge[2] = {(idxX ? addr - 1 : -1), (rowBaseAddr ? addr - m_numCuInWidth : -1)};
 
    // 是否存在色度平面 
    bool chroma = m_param->internalCsp != X265_CSP_I400 && 
                  m_frame->m_fencPic->m_picCsp != X265_CSP_I400;
 
    // 我们只考虑I420格式，因此存在色度平面，因此此处planes取值3
    int planes = chroma ? 3 : 1;    
 
    // 选择SAO方法处理去方块边界像素，如果开启则处理所有边界像素，
    // 关闭则不处理右边和下面边界的像素；缺省是关闭。
    if (m_param->bSaoNonDeblocked)
    {
        memcpy(m_count, m_countPreDblk[addr], sizeof(m_count));
        memcpy(m_offsetOrg, m_offsetOrgPreDblk[addr], sizeof(m_offsetOrg));
    }
    else
    {   // 初始化各模式各类型点的个数和失真值为0
        memset(m_count, 0, sizeof(m_count));
        memset(m_offsetOrg, 0, sizeof(m_offsetOrg));
    }
 
    for (int i = 0; i < planes; i++)
        saoParam->ctuParam[i][addr].reset();
 
    // 统计当前CTU的亮度块在BO和EO各模式下的像素归类，
    // 包括重构像素与原始像素差值之和，以及对classIdx的计数
    if (saoParam->bSaoFlag[0])
        calcSaoStatsCTU(addr, 0);
 
    // 统计当前CTU的色度块在BO和EO各模式下的像素归类
    if (saoParam->bSaoFlag[1])
    {
        calcSaoStatsCTU(addr, 1);
        calcSaoStatsCTU(addr, 2);
    }
 
    // 利用上一步的统计信息计算BO和EO初始补偿值
    saoStatsInitialOffset(planes);
 
    // SAO distortion calculation
    m_entropyCoder.load(m_rdContexts.cur);
    m_entropyCoder.resetBits();
    if (allowMerge[0])
        m_entropyCoder.codeSaoMerge(0);
    if (allowMerge[1])
        m_entropyCoder.codeSaoMerge(0);
    m_entropyCoder.store(m_rdContexts.temp);
 
    // Estimate distortion and cost of new SAO params
    int64_t bestCost = 0;
    int64_t rateDist = 0;
 
    // Estimate distortion and cost of new SAO params
    // 亮度和色度最优SAO模式的选择，得到最优率失真代价
    saoLumaComponentParamDist(saoParam, addr, rateDist, lambda, bestCost);
    if (chroma)
        saoChromaComponentParamDist(saoParam, addr, rateDist, lambda, bestCost);
 
    if (saoParam->bSaoFlag[0] || saoParam->bSaoFlag[1])
    {
        // Cost of merge left or Up， mergeIdx为0表示左边，为1表示上面
        // 计算直接采用左边和上面CTU的SAO参数的代价
        for (int mergeIdx = 0; mergeIdx < 2; ++mergeIdx)
        {
            // 如果左边或上面的CTU不存在，则跳过下面的计算，进入下一轮循环
            if (!allowMerge[mergeIdx])
                continue;
 
            int64_t mergeDist = 0; 
            for (int plane = 0; plane < planes; plane++)
            {
                // 初始失真值为0，获取左边或上面CTU的SAO参数
                int64_t estDist = 0;
                SaoCtuParam* mergeSrcParam = &(saoParam->ctuParam[plane][addrMerge[mergeIdx]]);
                int typeIdx = mergeSrcParam->typeIdx;
                if (typeIdx >= 0)
                {
                    // 如果是边带模式，获取第一个条带的编号；否则取值1
                    int bandPos = (typeIdx == SAO_BO) ? mergeSrcParam->bandPos : 1;
                    for (int classIdx = 0; classIdx < SAO_NUM_OFFSET; classIdx++)
                    {
                        // 根据4种类型的补偿值来计算失真差值
                        int mergeOffset = mergeSrcParam->offset[classIdx];
                        estDist += estSaoDist(m_count[plane][typeIdx][classIdx + bandPos], mergeOffset, 
                                                m_offsetOrg[plane][typeIdx][classIdx + bandPos]);
                    }
                }
                mergeDist += (estDist << 8) / lambda[!!plane];
            }
 
            m_entropyCoder.load(m_rdContexts.cur);
            m_entropyCoder.resetBits();
            if (allowMerge[0])
                m_entropyCoder.codeSaoMerge(1 - mergeIdx);
            if (allowMerge[1] && (mergeIdx == 1))
                m_entropyCoder.codeSaoMerge(1);
 
            uint32_t estRate = m_entropyCoder.getNumberOfWrittenBits();
            int64_t mergeCost = mergeDist + estRate;
            if (mergeCost < bestCost)
            {
                // merge的代价比SAO各模式代价更小，就采用merge模式
                SaoMergeMode mergeMode = mergeIdx ? SAO_MERGE_UP : SAO_MERGE_LEFT;
                bestCost = mergeCost;
                m_entropyCoder.store(m_rdContexts.temp);
                for (int plane = 0; plane < planes; plane++)
                {
                    // 更新SAO参数为merge模式下的参数
                    if (saoParam->bSaoFlag[plane > 0])
                    {
                        SaoCtuParam* dstCtuParam   = &saoParam->ctuParam[plane][addr];
                        SaoCtuParam* mergeSrcParam = &(saoParam->ctuParam[plane][addrMerge[mergeIdx]]);
                        dstCtuParam->mergeMode = mergeMode;
                        dstCtuParam->typeIdx   = mergeSrcParam->typeIdx;
                        dstCtuParam->bandPos   = mergeSrcParam->bandPos;
 
                        for (int i = 0; i < SAO_NUM_OFFSET; i++)
                            dstCtuParam->offset[i] = mergeSrcParam->offset[i];
                    }
                }
            }   // if (mergeCost < bestCost) 结束
        }   // mergeIdx循环结束
 
        if (saoParam->ctuParam[0][addr].typeIdx < 0)
            m_numNoSao[0]++;
        if (chroma && saoParam->ctuParam[1][addr].typeIdx < 0)
            m_numNoSao[1]++;
        m_entropyCoder.load(m_rdContexts.temp);
        m_entropyCoder.store(m_rdContexts.cur);
    }
}
 
/* 利用先前已经得到的统计信息（即m_count和m_offsetOrg）计算初始补偿值(即m_offset) */
void SAO::saoStatsInitialOffset(int planes)
{
    memset(m_offset, 0, sizeof(m_offset));
 
    // EO
    for (int plane = 0; plane < planes; plane++)
    {
        // typeIdx, 边界补偿的四种模式，即 SAO_EO_X
        for (int typeIdx = 0; typeIdx < MAX_NUM_SAO_TYPE - 1; typeIdx++)
        {
            // 任意一种模式下，边界点有四个种类
            for (int classIdx = 1; classIdx < SAO_NUM_OFFSET + 1; classIdx++)
            {
                int32_t&  count     = m_count[plane][typeIdx][classIdx];
                int32_t& offsetOrg = m_offsetOrg[plane][typeIdx][classIdx];
                int32_t& offsetOut = m_offset[plane][typeIdx][classIdx];
 
                if (count)
                {
                    // 计算平均失真（offsetOrg/count并四舍五入），将其限制在[-7,7]之内
                    offsetOut = roundIBDI(offsetOrg, count << SAO_BIT_INC);
                    offsetOut = x265_clip3(-OFFSET_THRESH + 1, OFFSET_THRESH - 1, offsetOut);
 
                    // 种类1、种类2的补偿值必须大于等于0；
                    // 种类3、种类4的补偿值必须小于等于0.
                    if (classIdx < 3) 
                        offsetOut = X265_MAX(offsetOut, 0);
                    else
                        offsetOut = X265_MIN(offsetOut, 0);
                }
            }
        }
    }
 
    // BO，为每个条带计算初始补偿值
    for (int plane = 0; plane < planes; plane++)
    {
        // 深度8位，256级分为32个边带，即[8k, 8k + 7]为第k个边带
        for (int classIdx = 0; classIdx < MAX_NUM_SAO_CLASS; classIdx++)
        {
            int32_t&  count     = m_count[plane][SAO_BO][classIdx];
            int32_t& offsetOrg = m_offsetOrg[plane][SAO_BO][classIdx];
            int32_t& offsetOut = m_offset[plane][SAO_BO][classIdx];
 
            if (count)
            {
                // 计算平均失真，并将其限制在[-7,7]之内
                offsetOut = roundIBDI(offsetOrg, count << SAO_BIT_INC);
                offsetOut = x265_clip3(-OFFSET_THRESH + 1, OFFSET_THRESH - 1, offsetOut);
            }
        }
    }
}
 
/* 计算率失真代价值，公式为：（失真 + lambda * 编码比特数）*/
inline int64_t SAO::calcSaoRdoCost(int64_t distortion, uint32_t bits, int64_t lambda)
{
    // lambda = 256.0 * x265_lambda2_tab[],所以需要右移8位，即除以256
    // 数组x265_lambda2_tab 定义在 common/constants.cpp 中
    return distortion + ((bits * lambda + 128) >> 8);
}
 
/**
 * @brief 找到最优率失真代价及对应的补偿值和失真值.
 * @param typeIdx : SAO模式，即 SAO_EO_X 和 SAO_BO
 * @param lambda  : 拉格朗日乘子，取值依赖QP，即 256.0 * x265_lambda2_tab[qp]
 * @param count   : typeIdx模式下，某classIdx的点的数目
 * @param offsetOrg : 原始像素与重构像素（SAO补偿前）之间的差值之和
 * @param offset[输出] : 最优率失真代价对应的补偿值
 * @param distClasses[输出] : 最优率失真代价对应的失真
 * @param costClasses[输出] : 最优率失真代价
 */
void SAO::estIterOffset(int typeIdx, int64_t lambda, int32_t count, int32_t offsetOrg, 
                        int32_t& offset, int32_t& distClasses, int64_t& costClasses)
{
    int bestOffset = 0;
    distClasses    = 0;
 
    // Assuming sending quantized value 0 results in zero offset 
    // and sending the value zero needs 1 bit.
    // entropy coder can be used to measure the exact rate here.
    int64_t bestCost = calcSaoRdoCost(0, 1, lambda);
    while (offset != 0)
    {
        // Calculate the bits required for signalling the offset
        uint32_t rate = (typeIdx == SAO_BO) ? (abs(offset) + 2) : (abs(offset) + 1);
        if (abs(offset) == OFFSET_THRESH - 1)
            rate--;
 
        // Do the dequntization before distorion calculation
        // 计算D_post和 D_pre的差值,即SAO补偿前后失真的差值
        int64_t dist = estSaoDist(count, offset << SAO_BIT_INC, offsetOrg);
 
        // 计算率失真代价
        int64_t cost  = calcSaoRdoCost(dist, rate, lambda);
        if (cost < bestCost)
        {
            bestCost = cost;
            bestOffset = offset;
            distClasses = (int)dist;
        }
        offset = (offset > 0) ? (offset - 1) : (offset + 1);
    }
 
    costClasses = bestCost;
    offset = bestOffset;
}
 
/* 寻找亮度最优SAO模式，得到最优率失真代价 */
void SAO::saoLumaComponentParamDist(SAOParam* saoParam, int32_t addr, int64_t& rateDist, 
                                    int64_t* lambda, int64_t &bestCost)
{
    int64_t bestDist = 0;
    int bestTypeIdx = -1;
    SaoCtuParam* lclCtuParam = &saoParam->ctuParam[0][addr];
 
    int32_t distClasses[MAX_NUM_SAO_CLASS];
    int64_t costClasses[MAX_NUM_SAO_CLASS];
 
    // RDO SAO_NA
    m_entropyCoder.load(m_rdContexts.temp);
    m_entropyCoder.resetBits();
    m_entropyCoder.codeSaoType(0);
 
    // 计算初始的率失真代价值
    int64_t costPartBest = calcSaoRdoCost(0, m_entropyCoder.getNumberOfWrittenBits(), lambda[0]);
 
    // EO distortion calculation
    // 外循环是EO的模式，即4种方向，内循环是点的种类
    for (int typeIdx = 0; typeIdx < MAX_NUM_SAO_TYPE - 1; typeIdx++)
    {
        int64_t estDist = 0;        // 用于保存某EO模式下各种类失真总和
        for (int classIdx = 1; classIdx < SAO_NUM_OFFSET + 1; classIdx++)
        {
            int32_t&  count     = m_count[0][typeIdx][classIdx];
            int32_t& offsetOrg = m_offsetOrg[0][typeIdx][classIdx];
            int32_t& offsetOut = m_offset[0][typeIdx][classIdx];
 
            // 计算率失真代价值最小的 offset
            estIterOffset(typeIdx, lambda[0], count, offsetOrg, offsetOut, 
                            distClasses[classIdx], costClasses[classIdx]);
 
            // Calculate distortion
            estDist += distClasses[classIdx];
        }
 
        m_entropyCoder.load(m_rdContexts.temp);
        m_entropyCoder.resetBits();
        m_entropyCoder.codeSaoOffsetEO(m_offset[0][typeIdx] + 1, typeIdx, 0);
 
        // 计算某EO模式下的率失真代价，
        // 如果比前面计算的更小，则更新最优率失真代和相应的EO模式值
        int64_t cost = calcSaoRdoCost(estDist, m_entropyCoder.getNumberOfWrittenBits(), lambda[0]);
        if (cost < costPartBest)
        {
            costPartBest = cost;
            bestDist = estDist;
            bestTypeIdx = typeIdx;
        }
    }
 
    // 找到了最优的EO模式，则将最优模式值和补偿值保存起来
    if (bestTypeIdx != -1)
    {
        lclCtuParam->mergeMode = SAO_MERGE_NONE;
        lclCtuParam->typeIdx = bestTypeIdx;
        lclCtuParam->bandPos = 0;
        for (int classIdx = 0; classIdx < SAO_NUM_OFFSET; classIdx++)
            lclCtuParam->offset[classIdx] = m_offset[0][bestTypeIdx][classIdx + 1];
    }
 
    // BO RDO，为每个条带计算最优率失真代价及对应的补偿值
    // costClasses 保存了每个条带的最优率失真代价
    int64_t estDist = 0;
    for (int classIdx = 0; classIdx < MAX_NUM_SAO_CLASS; classIdx++)
    {
        int32_t&  count    = m_count[0][SAO_BO][classIdx];
        int32_t& offsetOrg = m_offsetOrg[0][SAO_BO][classIdx];
        int32_t& offsetOut = m_offset[0][SAO_BO][classIdx];
 
        estIterOffset(SAO_BO, lambda[0], count, offsetOrg, offsetOut, 
                        distClasses[classIdx], costClasses[classIdx]);
    }
 
    // Estimate Best Position
    int64_t bestRDCostBO = MAX_INT64;
    int32_t bestClassBO  = 0;
 
    // 统计任意连续4个条带的最优率失真代价之和，找出值最小的连续4个条带
    for (int i = 0; i < MAX_NUM_SAO_CLASS - SAO_NUM_OFFSET + 1; i++)
    {
        int64_t currentRDCost = 0;
        for (int j = i; j < i + SAO_NUM_OFFSET; j++)
            currentRDCost += costClasses[j];
 
        if (currentRDCost < bestRDCostBO)
        {
            bestRDCostBO = currentRDCost;
            bestClassBO  = i;                   // 连续4个条带的起始条带编号
        }
    }
 
    // 计算最优的连续4个条带的失真之和
    estDist = 0;
    for (int classIdx = bestClassBO; classIdx < bestClassBO + SAO_NUM_OFFSET; classIdx++)
        estDist += distClasses[classIdx];
 
    m_entropyCoder.load(m_rdContexts.temp);
    m_entropyCoder.resetBits();
    m_entropyCoder.codeSaoOffsetBO(m_offset[0][SAO_BO] + bestClassBO, bestClassBO, 0);
 
    // 计算BO模式下的率失真代价
    int64_t cost = calcSaoRdoCost(estDist, m_entropyCoder.getNumberOfWrittenBits(), lambda[0]);
 
    // 如果BO模式下的率失真代价比上面EO模式下的率失真代价更小，则更新相应的SAO参数
    if (cost < costPartBest)
    {
        costPartBest = cost;
        bestDist = estDist;
 
        lclCtuParam->mergeMode = SAO_MERGE_NONE;
        lclCtuParam->typeIdx = SAO_BO;
        lclCtuParam->bandPos = bestClassBO;
        for (int classIdx = 0; classIdx < SAO_NUM_OFFSET; classIdx++)
            lclCtuParam->offset[classIdx] = m_offset[0][SAO_BO][classIdx + bestClassBO];
    }
 
    rateDist = (bestDist << 8) / lambda[0];
    m_entropyCoder.load(m_rdContexts.temp);
    m_entropyCoder.codeSaoOffset(*lclCtuParam, 0);
    m_entropyCoder.store(m_rdContexts.temp);
}
 
/* 寻找色度最优SAO模式，得到最优率失真代价 */
void SAO::saoChromaComponentParamDist(SAOParam* saoParam, int32_t addr, 
                int64_t& rateDist, int64_t* lambda, int64_t &bestCost);
 
/* 统计某个CU内条带点数目及失真之和，count和stats分别是条带点计数和失真之和 */
void saoCuStatsBO_c(const int16_t *diff, const pixel *rec, intptr_t stride, 
                    int endX, int endY, int32_t *stats, int32_t *count)
{
    const int boShift = X265_DEPTH - SAO_BO_BITS;
 
    for (int y = 0; y < endY; y++)
    {
        for (int x = 0; x < endX; x++)
        {
            int classIdx = rec[x] >> boShift;   // 条带编号
            stats[classIdx] += diff[x];         // 某条带失真累计
            count[classIdx]++;                  // 某条带点数目累计
        }
 
        diff += MAX_CU_SIZE;        // 下一行失真数据地址
        rec += stride;              // 当前CU的下一行重构图像地址
    }
}
 
/* 统计CU内的点在EO_0模式（水平方向）下的各种类点的数目及失真之和 */
void saoCuStatsE0_c(const int16_t *diff, const pixel *rec, intptr_t stride, 
                    int endX, int endY, int32_t *stats, int32_t *count)
{
    int32_t tmp_stats[SAO::NUM_EDGETYPE];
    int32_t tmp_count[SAO::NUM_EDGETYPE];
 
    memset(tmp_stats, 0, sizeof(tmp_stats));
    memset(tmp_count, 0, sizeof(tmp_count));
 
    for (int y = 0; y < endY; y++)
    {
        int signLeft = signOf(rec[0] - rec[-1]);        // 当前边界点的左符号
        for (int x = 0; x < endX; x++)
        {
            int signRight = signOf2(rec[x], rec[x + 1]);      // 当前边界点的右符号
            uint32_t edgeType = signRight + signLeft + 2;     // 边界点类型
            signLeft = -signRight;               // 当前点的右符号 = - 右边点的左符号
 
            // edgeType与真实的点种类转换关系就是数组 s_eoTable[]
            X265_CHECK(edgeType <= 4, "edgeType check failure\n");
            tmp_stats[edgeType] += diff[x];     // 该类型点的失真累计
            tmp_count[edgeType]++;              // 该类型点的数目累计
        }
 
        diff += MAX_CU_SIZE;
        rec += stride;
    }
 
    // 返回各类型点的失真之和和数目
    for (int x = 0; x < SAO::NUM_EDGETYPE; x++)
    {
        stats[SAO::s_eoTable[x]] += tmp_stats[x];
        count[SAO::s_eoTable[x]] += tmp_count[x];
    }
}
 
/* 统计CU内的点在EO_1模式（垂直方向）下的各种类点的数目及失真之和 */
void saoCuStatsE1_c(const int16_t *diff, const pixel *rec, intptr_t stride, 
    int8_t *upBuff1, int endX, int endY, int32_t *stats, int32_t *count);
 
/* 统计CU内的点在EO_2模式（135度方向）下的各种类点的数目及失真之和 */
void saoCuStatsE2_c(const int16_t *diff, const pixel *rec, intptr_t stride, int8_t 
    *upBuff1, int8_t *upBufft, int endX, int endY, int32_t *stats, int32_t *count);
/* 统计CU内的点在EO_3模式（45度方向）下的各种类点的数目及失真之和 */
void saoCuStatsE3_c(const int16_t *diff, const pixel *rec, intptr_t stride, 
    int8_t *upBuff1, int endX, int endY, int32_t *stats, int32_t *count);
```
